{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a698859",
   "metadata": {},
   "source": [
    "## Swapper model build test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0fe9d454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kackaobank1/Desktop/dev/FaceSwap/swappers/SmoothSwap\n",
      "train:\n",
      "  isTrain: false\n",
      "  end_epoch: 1000\n",
      "  batch_size: 512\n",
      "  optimizer: adam\n",
      "  weight_decay: 0.0005\n",
      "  momentum: 0.9\n",
      "  lr: 0.001\n",
      "  lr_step: 10\n",
      "  log_dir: ./logs\n",
      "  checkpoint_dir: ./checkpoints\n",
      "  device: cpu\n",
      "id_emb:\n",
      "  image_size: 224\n",
      "  train_embedder: false\n",
      "  network: resnet50\n",
      "  emb_size: 512\n",
      "  checkpoint_path: ''\n",
      "generator:\n",
      "  image_size: 256\n",
      "  num_feature_init: 64\n",
      "discriminator:\n",
      "  image_size: 256\n",
      "authur:\n",
      "  name: conor.k\n",
      "  version: 0.0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kackaobank1/.pyenv/versions/3.8.10/envs/python3.8/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from glob import glob\n",
    "# 이 파일의 Directory의 절대 경로\n",
    "__file__path = os.path.abspath(\"\")\n",
    "print(__file__path)\n",
    "sys.path.append(__file__path)\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Configuration \n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "config_path = \"./configs\"\n",
    "config_name = \"config.yaml\"\n",
    "try: hydra.initialize(version_base=None, \n",
    "                      config_path=config_path)\n",
    "except: pass\n",
    "cfg = hydra.compose(config_name=config_name)\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62143d67",
   "metadata": {},
   "source": [
    "### Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c3fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.id_emb import IdEmbedder\n",
    "from models.generator import Generator\n",
    "from models.discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de9a3fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceSwapModel(nn.Module):\n",
    "    def __init__(self, G, E, D):\n",
    "        super(FaceSwapModel, self).__init__()\n",
    "        self.G = G\n",
    "        self.D = D\n",
    "        self.E = E\n",
    "\n",
    "    def forward(self, x_src, x_tar):\n",
    "        z_src = self.E(x_src)\n",
    "        swap = self.G(x_tar, z_src)\n",
    "        disc = self.D(swap)\n",
    "        z_swap = self.E(swap)\n",
    "        return z_src, swap, z_swap, disc\n",
    "    \n",
    "    def get_discriminant(self, x):\n",
    "        return self.D(x)\n",
    "\n",
    "    def get_id_embedding(self, x):\n",
    "        return self.E(x)\n",
    "    \n",
    "    def get_swap_face(self, x_src, x_tar):\n",
    "        temb = self.E(x_src)\n",
    "        return self.G(x_tar, temb)\n",
    "    \n",
    "def cosine_metric(x1, x2):\n",
    "    return torch.sum(x1 * x2, dim=1) / (torch.norm(x1, dim=1) * torch.norm(x2, dim=1))\n",
    "\n",
    "class CombineLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    SmoothSwap Combined loss \n",
    "    \"\"\"\n",
    "    def __init__(self, lambdas=[4., 1., 1.]):\n",
    "        super(CombineLoss, self).__init__()\n",
    "        self.lambdas = lambdas\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-7)\n",
    "\n",
    "    def forward(self, x_tar, z_src, swap, z_swap, disc):\n",
    "        lambdas = [4., 1., 1.]\n",
    "        L_id  = torch.mean(1 - cosine_metric(z_src, z_swap))\n",
    "        L_chg = torch.mean((swap - x_tar).pow(2))\n",
    "#         L_adv = torch.mean(-1. * torch.log(disc))\n",
    "        L_adv = 0.\n",
    "        combined_loss = lambdas[0]*L_id + lambdas[1]*L_chg + lambdas[2]*L_adv\n",
    "        \n",
    "        return combined_loss, L_id, L_chg, L_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24b4637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsm.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0c842fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "class FFHQDataset(Dataset):\n",
    "    def __init__(self, image_pool, transform=None):\n",
    "        self.image_pool = image_pool\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_pool)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_src = Image.open(self.image_pool[idx])\n",
    "        img_tar = Image.open(np.random.choice(self.image_pool))\n",
    "        \n",
    "        if self.transform:\n",
    "            img_src = self.transform(img_src)\n",
    "            img_tar = self.transform(img_tar)\n",
    "        \n",
    "        return img_src, img_tar\n",
    "    \n",
    "def test_FFHQDataset(image_pool):\n",
    "    import matplotlib.pyplot as plt     \n",
    "    ffhq = FFHQDataset(image_pool)\n",
    "\n",
    "    for i in np.random.randint(len(ffhq), size=3):\n",
    "        sample = ffhq[i]\n",
    "    \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(sample[0])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(sample[1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dd4ad43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_batch\n",
    "def train_epoch(epoch,\n",
    "                train_loader, \n",
    "                model, \n",
    "                loss_fn, \n",
    "                optimizer, \n",
    "                optimizer_D, \n",
    "                cuda, \n",
    "                log_interval,\n",
    "                metrics):\n",
    "    \n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        img_src, img_tar = sample\n",
    "        # add target target sample\n",
    "        img_src[-1] = img_tar[-1]\n",
    "        \n",
    "        if cuda:\n",
    "            img_src = img_src.cuda()\n",
    "            img_tar = img_tar.cuda()\n",
    "            \n",
    "        # Discriminator\n",
    "        img_fake = model.get_swap_face(img_src, img_tar).detach()\n",
    "        gen_logit = model.D(img_fake)\n",
    "        loss_Dgen = (F.relu(torch.ones_like(gen_logit) + gen_logit)).mean()\n",
    "        \n",
    "        real_logit = model.D(img_tar)\n",
    "        loss_Dreal = (F.relu(torch.ones_like(real_logit) - real_logit)).mean()\n",
    "        loss_d = loss_Dgen + loss_Dreal\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_d.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Swapper\n",
    "        z_src, swap, z_swap, disc = model(img_src, img_tar)\n",
    "        loss_outputs = loss_fn(img_tar, z_src, swap, z_swap, disc)        \n",
    "        loss = torch.mean(loss_outputs[0])\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(outputs, loss_outputs)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx * len(img_src[0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
    "            \n",
    "            train_log = {\n",
    "                \"epoch\" : epoch,\n",
    "                \"batch\" : batch_idx,\n",
    "                'total_l': loss_outputs[0],\n",
    "                'id_l'   : loss_outputs[1],\n",
    "                'chg_l'  : loss_outputs[2],\n",
    "                'gan_l'  : loss_outputs[3],\n",
    "            }\n",
    "            wandb.log(train_log)\n",
    "            \n",
    "            message += \"\\t id_loss:{:.3f}, chg_loss:{:.3f}, gan_loss:{:.3f}\".format(*loss_outputs[1:])\n",
    "            \n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "            print(message, end=\"\\r\")\n",
    "            losses = []\n",
    "\n",
    "    total_loss /= (batch_idx + 1)\n",
    "    return total_loss, metrics\n",
    "\n",
    "\n",
    "def detransform_output(swap):\n",
    "    imagenet_std    = torch.Tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    imagenet_mean   = torch.Tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    swap = swap * imagenet_std\n",
    "    swap = swap + imagenet_mean\n",
    "    return swap.numpy()\n",
    "\n",
    "\n",
    "def test_epoch(val_loader, model, loss_fn, cuda, metrics):\n",
    "    with torch.no_grad():\n",
    "        for metric in metrics:\n",
    "            metric.reset()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for batch_idx, sample in enumerate(val_loader):\n",
    "            img_src, img_tar = sample\n",
    "            if cuda:\n",
    "                img_src = img_src.cuda()\n",
    "                img_tar = img_tar.cuda()\n",
    "                \n",
    "            outputs = model(*data)\n",
    "\n",
    "            loss_inputs = outputs\n",
    "            loss_outputs = loss_fn(*loss_inputs)\n",
    "            \n",
    "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            for metric in metrics:\n",
    "                metric(outputs, target, loss_outputs)\n",
    "                \n",
    "        img_src = detransform_output(img_src.to('cpu')).transpose(0,2,3,1)\n",
    "        img_tar = detransform_output(img_tar.to('cpu')).transpose(0,2,3,1)\n",
    "        swap = detransform_output(outputs[1]).transpose(0,2,3,1)\n",
    "        \n",
    "        table = wandb.Table(columns=[\"src\", \"tar\", \"swap\"])\n",
    "        for src, tar, swa in zip(img_src, img_tar, swap):\n",
    "            table.add_data(wandb.Image(src*255), wandb.Image(tar*255), wandb.Image(swa*255))\n",
    "        wandb.log({\"swap_sample\":table}, commit=False)\n",
    "                \n",
    "    return val_loss, metrics\n",
    "\n",
    "def fit(train_loader, \n",
    "        val_loader, \n",
    "        model, \n",
    "        loss_fn, \n",
    "        optimizer,\n",
    "        optimizer_D,\n",
    "        scheduler, \n",
    "        n_epochs, \n",
    "        cuda, \n",
    "        log_interval,\n",
    "        checkpoint_dir=\"./checkpoints/smoothswap\",\n",
    "        metrics=[],\n",
    "        start_epoch=0):\n",
    "\n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "\n",
    "        # Train stage\n",
    "        train_loss, metrics = train_epoch(epoch, train_loader, model, loss_fn, optimizer, optimizer_D, cuda, log_interval, metrics)\n",
    "        scheduler.step()\n",
    "        \n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "            \n",
    "        val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, \n",
    "                                                                                 n_epochs,\n",
    "                                                                                 val_loss)\n",
    "        val_log = {\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,            \n",
    "        }\n",
    "        wandb.log(val_log)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "        print(message)\n",
    "        state = {'epoch': epoch,\n",
    "                 'model': model,\n",
    "                 'optimizer': optimizer}\n",
    "        filename = os.path.join(checkpoint_dir, \n",
    "                                'cpt_' + str(epoch) + '_' + \"{:.4f}\".format(val_loss) + '.tar')\n",
    "        torch.save(state, filename)\n",
    "        print(\"{} is saved\".format(filename))\n",
    "        \n",
    "    wandb.finish()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b4d5a7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1cswbaao) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁</td></tr><tr><td>chg_l</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>gan_l</td><td>▁</td></tr><tr><td>id_l</td><td>▁</td></tr><tr><td>total_l</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>0</td></tr><tr><td>chg_l</td><td>0.03458</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>gan_l</td><td>0.0</td></tr><tr><td>id_l</td><td>0.52033</td></tr><tr><td>total_l</td><td>2.11588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fiery-bird-2</strong>: <a href=\"https://wandb.ai/conor-k/FaceSwap-SmoothSwap/runs/1cswbaao\" target=\"_blank\">https://wandb.ai/conor-k/FaceSwap-SmoothSwap/runs/1cswbaao</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220824_203835-1cswbaao/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1cswbaao). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kackaobank1/Desktop/dev/FaceSwap/swappers/SmoothSwap/wandb/run-20220824_203937-39x2p1ub</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/conor-k/FaceSwap-SmoothSwap/runs/39x2p1ub\" target=\"_blank\">dazzling-darkness-3</a></strong> to <a href=\"https://wandb.ai/conor-k/FaceSwap-SmoothSwap\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/conor-k/FaceSwap-SmoothSwap/runs/39x2p1ub?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x146bc4a30>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../../data/ffhq/images1024x1024/\"\n",
    "image_pool = glob(data_dir+\"/*/*.png\")\n",
    "test_FFHQDataset(image_pool)\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 2\n",
    "log_interval = 5\n",
    "margin = 1.\n",
    "cuda = torch.cuda.is_available()\n",
    "lr = 1e-3\n",
    "lr_d = 4e-3\n",
    "\n",
    "wandb.init(\n",
    "    project=\"FaceSwap-SmoothSwap\",\n",
    "    config={\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"log_interval\":log_interval,\n",
    "        \"margin\" : margin,\n",
    "        \"lr\": lr,\n",
    "        \"lr_d\": lr_d,\n",
    "        })\n",
    "# config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cae3465",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m(cfg)\n\u001b[1;32m      3\u001b[0m D \u001b[38;5;241m=\u001b[39m Discriminator(cfg\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mimage_size)\n\u001b[1;32m      4\u001b[0m E \u001b[38;5;241m=\u001b[39m IdEmbedder(cfg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "# model\n",
    "G = Generator(cfg)\n",
    "D = Discriminator(cfg.discriminator.image_size)\n",
    "E = IdEmbedder(cfg)\n",
    "if cfg.id_emb.checkpoint_path:\n",
    "    E.load_state_dict(cfg.id_emb.checkpoint_path)\n",
    "E.requires_grad_(False)\n",
    "E.eval()\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    E.cuda()\n",
    "    \n",
    "    \n",
    "fsm = FaceSwapModel(G, E, D)\n",
    "loss_fn = CombineLoss()\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(256),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_set = np.random.choice(image_pool, int(len(image_pool)*0.9))\n",
    "test_set  = np.array(list(set(image_pool) - set(train_set)))\n",
    "\n",
    "train_dataset = FFHQDataset(train_set, transform=transform)\n",
    "test_dataset  = FFHQDataset(test_set,  transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f49a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(fsm.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, batch_size, gamma=0.1, last_epoch=-1)\n",
    "optimizer_D= optim.Adam(fsm.D.parameters(), lr=lr_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c7d9cb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/63000 (0%)]\tLoss: 1.667585\t id_loss:0.406, chg_loss:0.045, gan_loss:0.000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(train_loader, val_loader, model, loss_fn, optimizer, optimizer_D, scheduler, n_epochs, cuda, log_interval, checkpoint_dir, metrics, start_epoch)\u001b[0m\n\u001b[1;32m    137\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, n_epochs):\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Train stage\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     train_loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    145\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Train set: Average loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_epochs, train_loss)\n",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, train_loader, model, loss_fn, optimizer, optimizer_D, cuda, log_interval, metrics)\u001b[0m\n\u001b[1;32m     26\u001b[0m     img_tar \u001b[38;5;241m=\u001b[39m img_tar\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Discriminator\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m img_fake \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_swap_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_tar\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     30\u001b[0m gen_logit \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mD(img_fake)\n\u001b[1;32m     31\u001b[0m loss_Dgen \u001b[38;5;241m=\u001b[39m (F\u001b[38;5;241m.\u001b[39mrelu(torch\u001b[38;5;241m.\u001b[39mones_like(gen_logit) \u001b[38;5;241m+\u001b[39m gen_logit))\u001b[38;5;241m.\u001b[39mmean()\n",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36mFaceSwapModel.get_swap_face\u001b[0;34m(self, x_src, x_tar)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_swap_face\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_src, x_tar):\n\u001b[1;32m     22\u001b[0m     temb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE(x_src)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/envs/python3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/dev/FaceSwap/swappers/SmoothSwap/models/generator.py:392\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x_tar, temb)\u001b[0m\n\u001b[1;32m    390\u001b[0m h \u001b[38;5;241m=\u001b[39m procedure[\u001b[38;5;241m1\u001b[39m](h, temb, skips[skip_idx])\n\u001b[1;32m    391\u001b[0m skip_idx \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 392\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mprocedure\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskips\u001b[49m\u001b[43m[\u001b[49m\u001b[43mskip_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m skip_idx \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    394\u001b[0m h \u001b[38;5;241m=\u001b[39m procedure[\u001b[38;5;241m3\u001b[39m](h, temb)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/envs/python3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/dev/FaceSwap/swappers/SmoothSwap/models/generator.py:108\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x, temb, skip)\u001b[0m\n\u001b[1;32m    106\u001b[0m h \u001b[38;5;241m=\u001b[39m nonlinearity(h)\n\u001b[1;32m    107\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(h)\n\u001b[0;32m--> 108\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_conv_shortcut:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/envs/python3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/envs/python3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/envs/python3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(train_loader, \n",
    "    test_loader,\n",
    "    fsm,\n",
    "    loss_fn, \n",
    "    optimizer,\n",
    "    optimizer_D,\n",
    "    scheduler, \n",
    "    n_epochs, \n",
    "    cuda, \n",
    "    log_interval)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104c451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953981d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884e517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "59024608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁</td></tr><tr><td>chg_l</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>gan_l</td><td>▁</td></tr><tr><td>id_l</td><td>▁</td></tr><tr><td>total_l</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>0</td></tr><tr><td>chg_l</td><td>0.04549</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>gan_l</td><td>0.0</td></tr><tr><td>id_l</td><td>0.40552</td></tr><tr><td>total_l</td><td>1.66759</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dazzling-darkness-3</strong>: <a href=\"https://wandb.ai/conor-k/FaceSwap-SmoothSwap/runs/39x2p1ub\" target=\"_blank\">https://wandb.ai/conor-k/FaceSwap-SmoothSwap/runs/39x2p1ub</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220824_203937-39x2p1ub/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c19b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
